{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97125f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "import fiona\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "import rasterio\n",
    "from rasterstats import zonal_stats\n",
    "import numpy as np\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b45a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "lcz_data = {\n",
    "    'LCZ' : list(range(1, 18)),\n",
    "    'lcz_code': [\n",
    "        'LCZ 1', 'LCZ 2', 'LCZ 3', 'LCZ 4', 'LCZ 5', 'LCZ 6', 'LCZ 7', 'LCZ 8', 'LCZ 9', 'LCZ 10',\n",
    "        'LCZ 11 (A)', 'LCZ 12 (B)', 'LCZ 13 (C)', 'LCZ 14 (D)', 'LCZ 15 (E)', 'LCZ 16 (F)', 'LCZ 17 (G)'\n",
    "    ],\n",
    "    'description': [\n",
    "        'Compact highrise', 'Compact midrise', 'Compact lowrise', 'Open highrise', 'Open midrise',\n",
    "        'Open lowrise', 'Lightweight low-rise', 'Large lowrise', 'Sparsely built', 'Heavy Industry',\n",
    "        'Dense trees', 'Scattered trees', 'Bush, scrub', 'Low plants', 'Bare rock or paved',\n",
    "        'Bare soil or sand', 'Water'\n",
    "    ],\n",
    "    'color': [\n",
    "        '#910613', '#D9081C', '#FF0A22', '#C54F1E', '#FF6628', '#FF985E', '#FDED3F', '#BBBBBB',\n",
    "        '#FFCBAB', '#565656', '#006A18', '#00A926', '#628432', '#B5DA7F', '#000000', '#FCF7B1',\n",
    "        '#656BFA'\n",
    "    ]\n",
    "}\n",
    "\n",
    "lcz_label_df = pd.DataFrame(lcz_data)\n",
    "\n",
    "print(lcz_label_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27af0463",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_grid_data_by_city(\n",
    "        grid_path, \n",
    "        csv_path, \n",
    "        lcz_path, \n",
    "        lst_files, \n",
    "        output_dir, \n",
    "        id_field='id', \n",
    "        exclude_layers=None, \n",
    "        lcz_label_df=lcz_label_df):\n",
    "    \"\"\"\n",
    "    Process grid data from a GeoPackage and CSV for multiple layers,\n",
    "    creating separate GeoJSON files for each city.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    grid_path : str\n",
    "        Path to the GeoPackage file containing the grid layers\n",
    "    csv_path : str\n",
    "        Path to the CSV file containing data associated with each grid ID\n",
    "    lcz_path : str\n",
    "        Path to the LCZ classification file\n",
    "    lst_files : dict\n",
    "        Dictionary mapping city names to LST file paths, e.g.,\n",
    "        {'zurich': 'path/to/zurich.tif', 'geneva': 'path/to/geneva.tif'}\n",
    "    output_dir : str\n",
    "        Directory where the output GeoJSON files will be saved\n",
    "    id_field : str\n",
    "        The field name used to join the datasets\n",
    "    exclude_layers : list, optional\n",
    "        List of layer names to exclude from processing\n",
    "    \"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Initialize exclude_layers as empty list if None\n",
    "    if exclude_layers is None:\n",
    "        exclude_layers = []\n",
    "    \n",
    "    # Get all available layers\n",
    "    available_layers = fiona.listlayers(grid_path)\n",
    "    \n",
    "    # Filter out excluded layers\n",
    "    available_layers = [layer for layer in available_layers if layer not in exclude_layers]\n",
    "    \n",
    "    lcz_df = pd.read_csv(lcz_path)\n",
    "    lcz_df = lcz_df[[id_field, 'City', 'LCZ']].copy()\n",
    "    lcz_df = lcz_df.merge(lcz_label_df, left_on='LCZ', right_on='LCZ', how='left')\n",
    "\n",
    "    # Read the CSV data once\n",
    "    full_data_df = pd.read_csv(csv_path)\n",
    "    full_data_df = full_data_df.merge(lcz_df, on=[id_field, 'City'], how='left')\n",
    "    \n",
    "    # Group layers by city\n",
    "    geneva_layers = []\n",
    "    zurich_layers = []\n",
    "    \n",
    "    for layer in available_layers:\n",
    "        # Normalize layer name to lowercase for comparison\n",
    "        layer_lower = layer.lower()\n",
    "        if 'geneva' in layer_lower:\n",
    "            geneva_layers.append(layer)\n",
    "        elif 'zurich' in layer_lower:\n",
    "            zurich_layers.append(layer)\n",
    "    \n",
    "    # Process Geneva layers\n",
    "    if geneva_layers:\n",
    "        # Filter CSV data for Geneva only\n",
    "        geneva_data_df = full_data_df[full_data_df['City'].str.lower() == 'geneva'].copy()\n",
    "        print(f\"Filtered {len(geneva_data_df)} Geneva records from CSV data\")\n",
    "        \n",
    "        lst_file = lst_files.get('geneva')\n",
    "        process_city_layers(\n",
    "            grid_path, \n",
    "            geneva_data_df, \n",
    "            lst_file,  # Pass single file path instead of folder\n",
    "            geneva_layers, \n",
    "            os.path.join(output_dir, \"geneva_grid_data.geojson\"), \n",
    "            id_field, \n",
    "            city_suffix=\"geneva\"\n",
    "        )\n",
    "    \n",
    "    # Process Zurich layers\n",
    "    if zurich_layers:\n",
    "        # Filter CSV data for Zurich only\n",
    "        zurich_data_df = full_data_df[full_data_df['City'].str.lower() == 'zurich'].copy()\n",
    "        print(f\"Filtered {len(zurich_data_df)} Zurich records from CSV data\")\n",
    "\n",
    "        lst_file = lst_files.get('zurich')\n",
    "        process_city_layers(\n",
    "            grid_path, \n",
    "            zurich_data_df, \n",
    "            lst_file,  # Pass single file path instead of folder\n",
    "            zurich_layers,\n",
    "            os.path.join(output_dir, \"zurich_grid_data.geojson\"),\n",
    "            id_field, \n",
    "            city_suffix=\"zurich\"\n",
    "        )\n",
    "    \n",
    "    return geneva_layers, zurich_layers\n",
    "\n",
    "def clean_layer_name(layer_name, city_suffix):\n",
    "    \"\"\"\n",
    "    Extract the base layer name without city suffixes.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    layer_name : str\n",
    "        Original layer name from GeoPackage\n",
    "    city_suffix : str\n",
    "        City suffix to remove (e.g., 'geneva', 'zurich')\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "        Cleaned layer name without city-specific parts\n",
    "    \"\"\"\n",
    "    # Convert to lowercase for case-insensitive matching\n",
    "    layer_lower = layer_name.lower()\n",
    "    city_suffix = city_suffix.lower()\n",
    "    \n",
    "    # Remove common prefixes or suffixes\n",
    "    cleaned_name = layer_name\n",
    "    \n",
    "    # Pattern 1: Remove \"_cityname\"\n",
    "    cleaned_name = re.sub(f\"_{city_suffix}\", \"\", cleaned_name, flags=re.IGNORECASE)\n",
    "\n",
    "    # Pattern 2: Remove \"-cityname\"\n",
    "    cleaned_name = re.sub(f\"-{city_suffix}\", \"\", cleaned_name, flags=re.IGNORECASE)\n",
    "\n",
    "    return cleaned_name\n",
    "\n",
    "def calculate_lst_averages(grid_gdf, lst_file_path):\n",
    "    \"\"\"\n",
    "    Calculate LST values for each grid cell from a single GeoTIFF file.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    grid_gdf : GeoDataFrame\n",
    "        GeoDataFrame containing grid cells\n",
    "    lst_file_path : str or Path\n",
    "        Path to the LST GeoTIFF file\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    GeoDataFrame\n",
    "        Input GeoDataFrame with new 'lst_mean' column\n",
    "    \"\"\"\n",
    "    lst_file_path = Path(lst_file_path)\n",
    "    \n",
    "    if not lst_file_path.exists():\n",
    "        print(f\"LST file not found: {lst_file_path}\")\n",
    "        return grid_gdf\n",
    "    \n",
    "    print(f\"Processing LST file: {lst_file_path.name}\")\n",
    "    \n",
    "    try:\n",
    "        with rasterio.open(lst_file_path) as src:\n",
    "            # Calculate zonal statistics for each grid cell\n",
    "            stats = zonal_stats(\n",
    "                grid_gdf.geometry,\n",
    "                src.read(1),  # Read first band\n",
    "                affine=src.transform,\n",
    "                stats=['mean'],\n",
    "                nodata=src.nodata\n",
    "            )\n",
    "            \n",
    "            # Extract mean values and handle None values\n",
    "            means = []\n",
    "            for stat in stats:\n",
    "                if stat is not None and 'mean' in stat and stat['mean'] is not None:\n",
    "                    means.append(float(stat['mean']) - 273.15)  # Convert Kelvin to Celsius\n",
    "                else:\n",
    "                    means.append(np.nan)\n",
    "    \n",
    "            # Add to GeoDataFrame\n",
    "            grid_gdf = grid_gdf.copy()\n",
    "            grid_gdf['LST_mean'] = means\n",
    "            \n",
    "            # Add validation info\n",
    "            valid_count = np.sum(~np.isnan(means))\n",
    "            print(f\"  Found {valid_count} cells with valid LST values\")\n",
    "            \n",
    "            return grid_gdf\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing LST file: {e}\")\n",
    "        return grid_gdf\n",
    "\n",
    "def process_city_layers(grid_path, data_df, lst_file, layers, output_path, id_field, city_suffix=\"\"):\n",
    "    \"\"\"\n",
    "    Process and merge multiple layers for a city into a single GeoJSON with one feature per grid cell.\n",
    "    Dynamically uses the spatial join keys that are present in each layer.\n",
    "    \"\"\"\n",
    "    print(f\"Processing layers for {os.path.basename(output_path)}:\")\n",
    "    \n",
    "    # All potential spatial properties to use for merging layers\n",
    "    all_spatial_join_keys = ['id', 'left', 'top', 'right', 'bottom', 'row_index', 'col_index', 'geometry', 'typology']\n",
    "    \n",
    "    # Dictionary to store each layer's GeoDataFrame\n",
    "    layer_gdfs = {}\n",
    "    \n",
    "    # Track the CRS for consistent projection\n",
    "    common_crs = None\n",
    "    \n",
    "    # First, read all layers\n",
    "    for layer in layers:\n",
    "        print(f\"  - Reading layer: {layer}\")\n",
    "        try:\n",
    "            # Read the layer\n",
    "            layer_gdf = gpd.read_file(grid_path, layer=layer)\n",
    "            \n",
    "            # Store CRS for later use\n",
    "            if common_crs is None:\n",
    "                common_crs = layer_gdf.crs\n",
    "                \n",
    "            # Clean the layer name to remove city suffix\n",
    "            clean_name = clean_layer_name(layer, city_suffix)\n",
    "            \n",
    "            # Identify which spatial join keys are present in this layer\n",
    "            available_keys = [key for key in all_spatial_join_keys if key in layer_gdf.columns]\n",
    "            \n",
    "            # Store the available keys with the layer\n",
    "            layer_info = {\n",
    "                'gdf': layer_gdf,\n",
    "                'available_keys': available_keys\n",
    "            }\n",
    "            \n",
    "            # Store in dictionary\n",
    "            layer_gdfs[clean_name] = layer_info\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"    Error processing layer {layer}: {str(e)}\")\n",
    "    \n",
    "    # If no layers were read successfully, return\n",
    "    if not layer_gdfs:\n",
    "        print(f\"No layers could be processed for {os.path.basename(output_path)}\")\n",
    "        return\n",
    "    \n",
    "    # Start with the first layer as our base\n",
    "    base_layer_name = list(layer_gdfs.keys())[0]\n",
    "    combined_gdf = layer_gdfs[base_layer_name]['gdf'].copy()\n",
    "    \n",
    "    # Merge remaining layers one by one\n",
    "    for layer_name, layer_info in list(layer_gdfs.items())[1:]:\n",
    "        print(f\"  - Merging layer: {layer_name}\")\n",
    "        \n",
    "        # Get the layer GeoDataFrame\n",
    "        layer_gdf = layer_info['gdf']\n",
    "        \n",
    "        # Determine which keys to use for merging with this layer\n",
    "        # Find keys that exist in both the combined data and this layer\n",
    "        available_keys = layer_info['available_keys']\n",
    "        merge_keys = [key for key in available_keys if key in combined_gdf.columns]\n",
    "        \n",
    "        # Ensure we have at least some keys for joining, including 'geometry'\n",
    "        if 'geometry' not in merge_keys:\n",
    "            merge_keys.append('geometry')\n",
    "        \n",
    "        print(f\"    Using merge keys: {merge_keys}\")\n",
    "        \n",
    "        # Use outer join to keep all features\n",
    "        combined_gdf = combined_gdf.merge(\n",
    "            layer_gdf,\n",
    "            on=merge_keys,\n",
    "            how='outer'\n",
    "        )\n",
    "    \n",
    "    # Add a column to track which city this is\n",
    "    combined_gdf['City'] = city_suffix.lower()\n",
    "    \n",
    "    # Now join with the CSV data on id_field\n",
    "    if id_field in combined_gdf.columns:\n",
    "        print(f\"  - Joining with CSV data on field: {id_field}\")\n",
    "        combined_gdf = combined_gdf.merge(data_df, on=id_field, how='left')\n",
    "    else:\n",
    "        print(f\"  Warning: Combined data does not contain id_field '{id_field}', skipping CSV join\")\n",
    "    \n",
    "    # Add LST data if available\n",
    "    if os.path.exists(lst_file):\n",
    "        print(f\"  - Calculating LST averages\")\n",
    "        combined_gdf = calculate_lst_averages(combined_gdf, lst_file)\n",
    "    \n",
    "    if 'solar_winter' in combined_gdf.columns:\n",
    "        print(f\"  - Renaming solar_winter to Irradiance_W\")\n",
    "        combined_gdf = combined_gdf.rename(columns={'solar_winter': 'Irradiance_W'})\n",
    "    elif 'solar_winter_2' in combined_gdf.columns:\n",
    "        print(f\"  - Renaming solar_winter_2 to Irradiance_W\")\n",
    "        combined_gdf = combined_gdf.rename(columns={'solar_winter_2': 'Irradiance_W'})\n",
    "\n",
    "    if 'solar_summer' in combined_gdf.columns:\n",
    "        print(f\"  - Renaming solar_summer to Irradiance_S\")\n",
    "        combined_gdf = combined_gdf.rename(columns={'solar_summer': 'Irradiance_S'})\n",
    "\n",
    "    # Round all floating-point columns to 2 decimal places\n",
    "    float_columns = combined_gdf.select_dtypes(include=['float64']).columns\n",
    "    if len(float_columns) > 0:\n",
    "        print(f\"  - Rounding {len(float_columns)} float columns to 2 decimal places\")\n",
    "        combined_gdf[float_columns] = combined_gdf[float_columns].round(2)\n",
    "\n",
    "    # Export to GeoJSON\n",
    "    combined_gdf.to_file(output_path, driver=\"GeoJSON\")\n",
    "    print(f\"Exported {len(combined_gdf)} unique grid cells to {output_path}\")\n",
    "    \n",
    "    return combined_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f42d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "load_dotenv()\n",
    "\n",
    "data_folder = os.getenv('CITYTHERM_DATA_FOLDER', str(Path.home() / \"Data\"))\n",
    "grid_path = os.path.join(data_folder, \"open data multidomain neighbourhood types and environmental quality.gpkg\")\n",
    "csv_path = os.path.join(data_folder, \"open_data_neighbourhood_parameters.csv\")\n",
    "output_path = os.path.join(data_folder, \"grid_data.geojson\")\n",
    "lcz_path = os.path.join(data_folder, \"citytherm.csv\")\n",
    "updated_lcz_path = os.path.join(data_folder, \"citytherm_updated.csv\")\n",
    "lst_folder = os.path.join(data_folder, \"lst\")\n",
    "\n",
    "lst_files = {\n",
    "    'zurich': os.path.join(lst_folder, 'LST-zurich-19-7-24.tif'),\n",
    "    'geneva': os.path.join(lst_folder, 'LST-geneva-23-8-11.tif')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3d5db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lcz_data = pd.read_csv(lcz_path)\n",
    "water_cells = [11, 311, 312, 330, 331, 332, 333, 334, 335, 336, 337, 362, 386, 435]\n",
    "lcz_data.loc[lcz_data['id'].isin(water_cells), 'LCZ'] = 17\n",
    "lcz_data.loc[lcz_data['id'] == 165, 'LCZ'] = 2\n",
    "\n",
    "open_lowrise_cells = [39, 64, 89]\n",
    "lcz_data.loc[lcz_data['id'].isin(open_lowrise_cells), 'LCZ'] = 6\n",
    "\n",
    "lcz_data.to_csv(updated_lcz_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87b4b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "lcz_data.loc[lcz_data['id'].isin(water_cells)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae3be13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "output_dir = os.path.join(data_folder, \"processed\")\n",
    "\n",
    "# Process all layers by city\n",
    "geneva_layers, zurich_layers = process_grid_data_by_city(\n",
    "    grid_path=grid_path,\n",
    "    csv_path=csv_path,\n",
    "    lcz_path=updated_lcz_path,\n",
    "    lst_files=lst_files,\n",
    "    output_dir=output_dir,\n",
    "    id_field='id',\n",
    "    exclude_layers=[\"LST-Geneva\"]  # Exclude problematic layer\n",
    ")\n",
    "\n",
    "print(\"\\nProcessed Geneva layers:\")\n",
    "for layer in geneva_layers:\n",
    "    print(f\"- {layer}\")\n",
    "\n",
    "print(\"\\nProcessed Zurich layers:\")\n",
    "for layer in zurich_layers:\n",
    "    print(f\"- {layer}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "citytherm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
